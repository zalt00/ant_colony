{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28ddbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac28bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mylib\n",
    "import importlib\n",
    "mylib = importlib.reload(mylib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53df73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "class EdgePolicy(torch.nn.Module):\n",
    "    def __init__(self, node_feat_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # MLP pour GINEConv\n",
    "        gin_nn1 = Sequential(Linear(node_feat_dim, hidden_dim),\n",
    "                             ReLU(),\n",
    "                             Linear(hidden_dim, hidden_dim))\n",
    "        gin_nn2 = Sequential(Linear(hidden_dim, hidden_dim),\n",
    "                             ReLU(),\n",
    "                             Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.conv1 = GINEConv(gin_nn1, edge_dim=2)\n",
    "        self.conv2 = GINEConv(gin_nn2, edge_dim=2)\n",
    "\n",
    "        # head pour scorer chaque arête\n",
    "        self.edge_mlp = Sequential(\n",
    "            Linear(2*hidden_dim + 2, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # 1. Message passing\n",
    "        h = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "\n",
    "        # 2. Préparer embeddings d'arêtes\n",
    "        src, dst = edge_index\n",
    "        h_edge = torch.cat([h[src], h[dst], edge_attr], dim=-1)\n",
    "\n",
    "        # 3. Scores bruts\n",
    "        logits = self.edge_mlp(h_edge).squeeze(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7fb459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = mylib.Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c157aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des arêtes\n",
    "edges = helper.get_dataset()\n",
    "num_nodes = max(max(u,v) for u,v,t in edges) + 1\n",
    "\n",
    "# Séparer arrays pour plus de rapidité\n",
    "edge_index = torch.tensor([[u for u,v,t in edges],\n",
    "                           [v for u,v,t in edges]], dtype=torch.long)\n",
    "edge_types = torch.tensor([t for u,v,t in edges], dtype=torch.long)\n",
    "\n",
    "edge_to_id = {frozenset((u, v)): i for (i, (u, v, _)) in enumerate(edges)}\n",
    "\n",
    "# Node features initiales (ici, vecteurs unité)\n",
    "node_feat_dim = 1\n",
    "x_init = torch.ones((num_nodes, node_feat_dim), dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04cf64a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 050 | Réward cumulé 0.24\n",
      "Épisode 100 | Réward cumulé 0.31\n",
      "Épisode 150 | Réward cumulé 0.34\n",
      "Épisode 200 | Réward cumulé 0.44\n",
      "Épisode 250 | Réward cumulé 0.50\n",
      "Épisode 300 | Réward cumulé 0.12\n",
      "Épisode 350 | Réward cumulé 0.43\n",
      "Épisode 400 | Réward cumulé 0.16\n",
      "Épisode 450 | Réward cumulé 0.35\n",
      "Épisode 500 | Réward cumulé 0.24\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "# Hyperparamètres\n",
    "hidden_dim = 64\n",
    "lr = 1e-3\n",
    "episodes = 500\n",
    "max_steps = 20 # len(edges)  # au pire on flippes toutes les arêtes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EdgePolicy(node_feat_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for ep in range(episodes):\n",
    "    # Réinitialiser l'état\n",
    "    helper.reset(ep)\n",
    "    edges = helper.get_dataset()\n",
    "    edge_index = torch.tensor([[u for u,v,t in edges],\n",
    "                           [v for u,v,t in edges]], dtype=torch.long)\n",
    "\n",
    "    edge_to_id = {frozenset((u, v)): i for (i, (u, v, _)) in enumerate(edges)}\n",
    "    types = torch.tensor([t for u,v,t in edges], dtype=torch.long).to(device)\n",
    "\n",
    "    x = x_init.to(device)\n",
    "\n",
    "    episode_loss = 0.0\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Construire edge_attr sous forme one-hot\n",
    "        edge_attr = F.one_hot(types, num_classes=2).to(torch.float)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(x, edge_index.to(device), edge_attr)\n",
    "\n",
    "        # Masque arêtes de type 0\n",
    "        mask0 = (types == 0).to(device)\n",
    "        if mask0.sum() == 0:\n",
    "            break  # plus d'arêtes à flipper\n",
    "\n",
    "        logits0 = logits[mask0]\n",
    "        probs0 = F.softmax(logits0, dim=0)\n",
    "        dist = Categorical(probs0)\n",
    "\n",
    "        # Échantillonnage de l’action\n",
    "        a_idx = dist.sample()  # indice dans les arêtes type0\n",
    "        logp = dist.log_prob(a_idx)\n",
    "\n",
    "        # Traduire en index global\n",
    "        global_idx = mask0.nonzero()[a_idx]\n",
    "\n",
    "        # Exécuter l’action : flip\n",
    "        (r, (u2, v2)) = helper.recompense(global_idx)  # récompense\n",
    "\n",
    "        # Mise à jour temporaire de l’état\n",
    "        types[global_idx] = 1\n",
    "        idx2 = edge_to_id[frozenset((u2, v2))]\n",
    "        types[idx2] = 0\n",
    "\n",
    "        # Accumuler perte et récompense\n",
    "        episode_loss += -logp * r\n",
    "        episode_reward += r\n",
    "\n",
    "    # Backprop et mise à jour des paramètres\n",
    "    optimizer.zero_grad()\n",
    "    episode_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (ep+1) % 50 == 0:\n",
    "        print(f\"Épisode {ep+1:03d} | Réward cumulé {episode_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e51af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} {} {} 0 0.006058865007443002 -0.0015014195239135182\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (559) must match the size of tensor b (568) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m edge_attr = F.one_hot(types, num_classes=\u001b[32m2\u001b[39m).to(torch.float)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Masque arêtes de type 0\u001b[39;00m\n\u001b[32m     22\u001b[39m mask0 = (types == \u001b[32m0\u001b[39m).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ALB\\Rp\\ant_colony\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ALB\\Rp\\ant_colony\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mEdgePolicy.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# 1. Message passing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     h = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     30\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.conv2(h, edge_index, edge_attr)\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# 2. Préparer embeddings d'arêtes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ALB\\Rp\\ant_colony\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ALB\\Rp\\ant_colony\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ALB\\Rp\\ant_colony\\venv\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py:187\u001b[39m, in \u001b[36mGINEConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, size)\u001b[39m\n\u001b[32m    184\u001b[39m     x = (x, x)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m x_r = x[\u001b[32m1\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gin_conv_GINEConv_propagate_d2vmo2oo.py:183\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, edge_attr, size)\u001b[39m\n\u001b[32m    174\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    175\u001b[39m                 x_j=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_j\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    176\u001b[39m                 edge_attr=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33medge_attr\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m                 dim_size=kwargs.dim_size,\n\u001b[32m    180\u001b[39m             )\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ALB\\Rp\\ant_colony\\venv\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py:204\u001b[39m, in \u001b[36mGINEConv.message\u001b[39m\u001b[34m(self, x_j, edge_attr)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    202\u001b[39m     edge_attr = \u001b[38;5;28mself\u001b[39m.lin(edge_attr)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mx_j\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m).relu()\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (559) must match the size of tensor b (568) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import random\n",
    "rm = 0.0\n",
    "rr = 0.0\n",
    "rc = 0\n",
    "for iter_id in range(50000):\n",
    "    helper.reset(iter_id)\n",
    "    edges = helper.get_dataset()\n",
    "    types = torch.tensor([t for u,v,t in edges], dtype=torch.long).to(device)\n",
    "\n",
    "    edge_index = torch.tensor([[u for u,v,t in edges],\n",
    "                           [v for u,v,t in edges]], dtype=torch.long)\n",
    "\n",
    "    edge_to_id = {frozenset((u, v)): i for (i, (u, v, _)) in enumerate(edges)}\n",
    "\n",
    "    x = x_init.to(device)\n",
    "\n",
    "    episode_loss = 0.0\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    # Construire edge_attr sous forme one-hot\n",
    "    edge_attr = F.one_hot(types, num_classes=2).to(torch.float)\n",
    "\n",
    "    # Forward\n",
    "    logits = model(x, edge_index.to(device), edge_attr)\n",
    "\n",
    "    # Masque arêtes de type 0\n",
    "    mask0 = (types == 0).to(device)\n",
    "    if mask0.sum() == 0:\n",
    "        break  # plus d'arêtes à flipper\n",
    "\n",
    "    logits0 = logits[mask0]\n",
    "    probs0 = F.softmax(logits0, dim=0)\n",
    "    dist = Categorical(probs0)\n",
    "\n",
    "    # Échantillonnage de l’action\n",
    "    a_idx = dist.sample()  # indice dans les arêtes type0\n",
    "    logp = dist.log_prob(a_idx)\n",
    "\n",
    "    # Traduire en index global\n",
    "    global_idx = mask0.nonzero()[a_idx]\n",
    "\n",
    "    global_idx2 = mask0.nonzero()[random.randrange(0, mask0.sum())]\n",
    "\n",
    "\n",
    "    # Exécuter l’action : flip\n",
    "    (r, (u2, v2)) = helper.recompense(global_idx)  # récompense\n",
    "\n",
    "    helper.reset(0)\n",
    "    (r2, (u2, v2)) = helper.recompense(global_idx2)  # récompense\n",
    "\n",
    "    rm += r\n",
    "    rr += r2\n",
    "    rc += 1\n",
    "    if iter_id % 1000 == 0.0:\n",
    "        print(\"{} {} {}\", iter_id, rm/rc, rr/rc)\n",
    "\n",
    "print(rm / rc, rr / rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23433bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71486dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
